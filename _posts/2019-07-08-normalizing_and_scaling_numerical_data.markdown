---
layout: post
title:      "Scrubbing Data"
date:       2019-07-08 00:36:53 -0400
permalink:  normalizing_and_scaling_numerical_data
---

      One of the steps in the OSEMN method is called "Scrub". This is sometimes the longest step in the entire process. The goal of the scrub step is to transform the data in a way that will make the final model more accurate. Before one beins this step make sure the data is in the form of a pandas dataframe. The scrub step can be broken down into parts. These parts include: casting data to the appropriate datatypes, Identifying and dealing with null values and duplicate values, checking for and dealing with multicolinearity, Normalizing and scaling the data, removing uneccesary columns,  removing outliers, and finally one hot encoding categorical features. Sometimes it is necessary to reiterate through the parts. Remember datas science is an iterative process.
	
      The first part of the scrub step is to cast columns to the appropriate data type. This can mean turning a column of strings into a column of integers, or turning an column of integers into a column of categories. This is very important because data needs to be of the right datatype in order to be used properly. One method for finding integer or float value arrays that need to be changed to categorial is to create a scatterplot for each column of data. use one predictor variable as the x value and the target variable as the y value. By checking if the dots in the scatter plot tend to stick to vertical lines you can determine if the data is categorial or continuous. Sometimes casting a column of data to a diffrent datatype must be delayed because there are inappropriate values such as null values. When this happens I simply make a note to cast the data to the appropriate datatype after I deal with the inappropriate values. Once all the data that can be cast to the appropriate data type is casted, It is time to move to the next part, Identifying and dealing with inappropriate values and duplicate values.
	
      The second part of the scrub step is identifying and dealing with inappropriate values and duplicate values. During the casting data to the apropriate datatype part it should become apparent in an error code if an array of data has an inappropriate value such as a question mark in a column full of numbers. Inappropriate values of this nature can be dealt with by using the .replace() method on a pandas series to replace the values with the median of the series (which can be found with the .median() method). Alternatively the rows can be dropped using the .drop() method, or even the entire series can be dropped if the inappropriate values take up too much of the column. Remember if a column could not be cast to the appropriate datatype in the previous part because of inappropriate values, then it may be able to be changed now. To Identify null values in a series use the .isna() method. This method will return a series filled with True / False values (True meaning the data is null). This can be used on a series with the .sum() method which will return the number of True values in the series. Then use the .replace() method to replace null values such as np.nan with the median of the series,  simply drop the rows with null values, or drop the whole column. To find duplicate values use the .duplicated() method on a dataframe and drop any duplicate rows. This brings us to the next step, removing unneccesary columns.
	
     The third part of the scrub step is checking for and dealing with multicolinearity. Multicolinearity is when predictor columns are highly correlated to each other. Essentialy using highly correlated predictors in a model can be like having two or more of the same column in the dataframe. One way to get a visual representation of multicolinearity is by creating a heat map. The documentation for the seaborn heat map method can be found here https://seaborn.pydata.org/generated/seaborn.heatmap.html
Another way of finding multicolinearity is by creating a small dataframe showing correlation as a decimal number. To do this use the .corr() method on a dataframe. There are spots in the dataframe where the correlation is equal to one because the program is checking for a correlation of one column to itself. Generally any column that has a correlation to another column that is greater than 0.75 should be considered highly correlated. To deal with this remove all of the highly correlated columns except for one. It might be best to explore the columns' distributions, Homoscedasticitys, and correlations to price before you decide which column to keep. After removing the columns it is time to go to the next part, normalizing and scaling the data.

      The fourth part of the scrub step is normalizing and scaling data. Normalizing data means transforming the data so that it more closely resembles a normal distribution. What I do to normalize data is first plot a histogram for every column including the target column. Then I create a log transformed version of every column. Next I create a histogram for the log transformed version of every column. I then compare the histograms of the data with and without log transformation and I decide which columns should be log transformed and which ones should not. Log transforming is a method used to make data more closely resemble a normal distribution. It can be done by simply using np.log(). Scaling data means to take all the data and make put it in the same number range. This way predictors with large numbers don't overpower predictors with small numbers. An easy way to scale data is by using the min-max technique. To perform a min-max one should first subtract the minimum value from every element in a series. Then one should divide each element in the series by the series' max minus the series' min. This concludes the normalization and standardization. The next step is removing unneccessary columns
	The fifth part of the scrub step is to remove unneccesary columns. This means to remove columns that just aren't relevent to the goal of the project. 
	
      The next part is to remove outliers. Sometimes this should be done later on after the scrub step. To remove outliers one can simply remove all rows that have a value in one of there continuous columns that is more than three standard deviations over the mean for that column or three standard deviations below the mean in that column. Check to see how many rows got removed. If a significant amount of rows were removed than one might want to wait untill after the scrub step when there are fewer continuous columns.
			
      The last Part of the scrub step is to one-hot encode categorial columns. One-hot encoding is essentially breaking each categorical column up so that there is a column for every category, and each of these columns contains ones and zeros representing whether or not a specific row of data contains a certain category. To do this use pd.get_dummies and set the proper parameters. (The documentation for pd.get_dummies can be found here https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html). Sometimes a person might want to turn a continous column into a categorical column. This requires binning. To bin just use pd.cut(). (pd.cut() documentation can be found here (https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html) When using pd.cut() set the bins parameter to how many categories you would like, or use one of the other ways to set bins as described in the documentation.
			
      These are all the parts to performing the scrub step. Remember the scrub step is often the longest step of the OSEMN process.





         
				 
				 
				 
				 
				 
